{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cb215a52-63f0-4339-8023-661cd29d052a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-4140719852775921>:5\u001B[0m\n",
       "\u001B[1;32m      2\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mpyspark\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m SparkContext, SparkConf\n",
       "\u001B[1;32m      4\u001B[0m conf \u001B[38;5;241m=\u001B[39m SparkConf()\u001B[38;5;241m.\u001B[39msetAppName(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mBigData\u001B[39m\u001B[38;5;124m'\u001B[39m)\n",
       "\u001B[0;32m----> 5\u001B[0m sc\u001B[38;5;241m=\u001B[39mSparkContext(conf\u001B[38;5;241m=\u001B[39mconf)\n",
       "\u001B[1;32m      6\u001B[0m \u001B[38;5;28mprint\u001B[39m(sc)\n",
       "\u001B[1;32m      7\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mReady to Go!!!!\u001B[39m\u001B[38;5;124m'\u001B[39m)\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/context.py:202\u001B[0m, in \u001B[0;36mSparkContext.__init__\u001B[0;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls, udf_profiler_cls, memory_profiler_cls)\u001B[0m\n",
       "\u001B[1;32m    196\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m gateway \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m gateway\u001B[38;5;241m.\u001B[39mgateway_parameters\u001B[38;5;241m.\u001B[39mauth_token \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
       "\u001B[1;32m    197\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n",
       "\u001B[1;32m    198\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mYou are trying to pass an insecure Py4j gateway to Spark. This\u001B[39m\u001B[38;5;124m\"\u001B[39m\n",
       "\u001B[1;32m    199\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m is not allowed as it is a security risk.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n",
       "\u001B[1;32m    200\u001B[0m     )\n",
       "\u001B[0;32m--> 202\u001B[0m \u001B[43mSparkContext\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_ensure_initialized\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgateway\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgateway\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mconf\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mconf\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m    203\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n",
       "\u001B[1;32m    204\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_do_init(\n",
       "\u001B[1;32m    205\u001B[0m         master,\n",
       "\u001B[1;32m    206\u001B[0m         appName,\n",
       "\u001B[0;32m   (...)\u001B[0m\n",
       "\u001B[1;32m    216\u001B[0m         memory_profiler_cls,\n",
       "\u001B[1;32m    217\u001B[0m     )\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/context.py:488\u001B[0m, in \u001B[0;36mSparkContext._ensure_initialized\u001B[0;34m(cls, instance, gateway, conf)\u001B[0m\n",
       "\u001B[1;32m    485\u001B[0m     callsite \u001B[38;5;241m=\u001B[39m SparkContext\u001B[38;5;241m.\u001B[39m_active_spark_context\u001B[38;5;241m.\u001B[39m_callsite\n",
       "\u001B[1;32m    487\u001B[0m     \u001B[38;5;66;03m# Raise error if there is already a running Spark context\u001B[39;00m\n",
       "\u001B[0;32m--> 488\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n",
       "\u001B[1;32m    489\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCannot run multiple SparkContexts at once; \u001B[39m\u001B[38;5;124m\"\u001B[39m\n",
       "\u001B[1;32m    490\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mexisting SparkContext(app=\u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m, master=\u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m)\u001B[39m\u001B[38;5;124m\"\u001B[39m\n",
       "\u001B[1;32m    491\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m created by \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m at \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m:\u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m \u001B[39m\u001B[38;5;124m\"\u001B[39m\n",
       "\u001B[1;32m    492\u001B[0m         \u001B[38;5;241m%\u001B[39m (\n",
       "\u001B[1;32m    493\u001B[0m             currentAppName,\n",
       "\u001B[1;32m    494\u001B[0m             currentMaster,\n",
       "\u001B[1;32m    495\u001B[0m             callsite\u001B[38;5;241m.\u001B[39mfunction,\n",
       "\u001B[1;32m    496\u001B[0m             callsite\u001B[38;5;241m.\u001B[39mfile,\n",
       "\u001B[1;32m    497\u001B[0m             callsite\u001B[38;5;241m.\u001B[39mlinenum,\n",
       "\u001B[1;32m    498\u001B[0m         )\n",
       "\u001B[1;32m    499\u001B[0m     )\n",
       "\u001B[1;32m    500\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
       "\u001B[1;32m    501\u001B[0m     SparkContext\u001B[38;5;241m.\u001B[39m_active_spark_context \u001B[38;5;241m=\u001B[39m instance\n",
       "\n",
       "\u001B[0;31mValueError\u001B[0m: Cannot run multiple SparkContexts at once; existing SparkContext(app=Databricks Shell, master=local[8]) created by __init__ at /databricks/python_shell/dbruntime/spark_connection.py:127 "
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)\nFile \u001B[0;32m<command-4140719852775921>:5\u001B[0m\n\u001B[1;32m      2\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mpyspark\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m SparkContext, SparkConf\n\u001B[1;32m      4\u001B[0m conf \u001B[38;5;241m=\u001B[39m SparkConf()\u001B[38;5;241m.\u001B[39msetAppName(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mBigData\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m----> 5\u001B[0m sc\u001B[38;5;241m=\u001B[39mSparkContext(conf\u001B[38;5;241m=\u001B[39mconf)\n\u001B[1;32m      6\u001B[0m \u001B[38;5;28mprint\u001B[39m(sc)\n\u001B[1;32m      7\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mReady to Go!!!!\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/context.py:202\u001B[0m, in \u001B[0;36mSparkContext.__init__\u001B[0;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls, udf_profiler_cls, memory_profiler_cls)\u001B[0m\n\u001B[1;32m    196\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m gateway \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m gateway\u001B[38;5;241m.\u001B[39mgateway_parameters\u001B[38;5;241m.\u001B[39mauth_token \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    197\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[1;32m    198\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mYou are trying to pass an insecure Py4j gateway to Spark. This\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    199\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m is not allowed as it is a security risk.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    200\u001B[0m     )\n\u001B[0;32m--> 202\u001B[0m \u001B[43mSparkContext\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_ensure_initialized\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgateway\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgateway\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mconf\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mconf\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    203\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m    204\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_do_init(\n\u001B[1;32m    205\u001B[0m         master,\n\u001B[1;32m    206\u001B[0m         appName,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    216\u001B[0m         memory_profiler_cls,\n\u001B[1;32m    217\u001B[0m     )\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/context.py:488\u001B[0m, in \u001B[0;36mSparkContext._ensure_initialized\u001B[0;34m(cls, instance, gateway, conf)\u001B[0m\n\u001B[1;32m    485\u001B[0m     callsite \u001B[38;5;241m=\u001B[39m SparkContext\u001B[38;5;241m.\u001B[39m_active_spark_context\u001B[38;5;241m.\u001B[39m_callsite\n\u001B[1;32m    487\u001B[0m     \u001B[38;5;66;03m# Raise error if there is already a running Spark context\u001B[39;00m\n\u001B[0;32m--> 488\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[1;32m    489\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCannot run multiple SparkContexts at once; \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    490\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mexisting SparkContext(app=\u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m, master=\u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m)\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    491\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m created by \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m at \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m:\u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    492\u001B[0m         \u001B[38;5;241m%\u001B[39m (\n\u001B[1;32m    493\u001B[0m             currentAppName,\n\u001B[1;32m    494\u001B[0m             currentMaster,\n\u001B[1;32m    495\u001B[0m             callsite\u001B[38;5;241m.\u001B[39mfunction,\n\u001B[1;32m    496\u001B[0m             callsite\u001B[38;5;241m.\u001B[39mfile,\n\u001B[1;32m    497\u001B[0m             callsite\u001B[38;5;241m.\u001B[39mlinenum,\n\u001B[1;32m    498\u001B[0m         )\n\u001B[1;32m    499\u001B[0m     )\n\u001B[1;32m    500\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    501\u001B[0m     SparkContext\u001B[38;5;241m.\u001B[39m_active_spark_context \u001B[38;5;241m=\u001B[39m instance\n\n\u001B[0;31mValueError\u001B[0m: Cannot run multiple SparkContexts at once; existing SparkContext(app=Databricks Shell, master=local[8]) created by __init__ at /databricks/python_shell/dbruntime/spark_connection.py:127 ",
       "errorSummary": "<span class='ansi-red-fg'>ValueError</span>: Cannot run multiple SparkContexts at once; existing SparkContext(app=Databricks Shell, master=local[8]) created by __init__ at /databricks/python_shell/dbruntime/spark_connection.py:127 ",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# Ititialize Spark content\n",
    "from pyspark import SparkContext, SparkConf\n",
    "\n",
    "conf = SparkConf().setAppName('BigData')\n",
    "sc=SparkContext(conf=conf)\n",
    "print(sc)\n",
    "print('Ready to Go!!!!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b7712cbb-5daf-4f32-b7f9-302e58d57acb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###  In case you are encounter with error \"Cannot run multiple SparkContexts at once\"\n",
    "### Run the following command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "03d04c72-d3ec-4a2d-bdc2-32d0c2d4f65c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "sc = SparkContext.getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7c148cbc-1f50-4014-a730-8fd69d6fc18b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Databricks Shell\n"
     ]
    }
   ],
   "source": [
    "# Get the Application Name:\n",
    "print(sc.appName)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b518b261-39e6-4c71-883c-661efc9365fe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "local[8]\n"
     ]
    }
   ],
   "source": [
    "# Get Master URL\n",
    "# This provides the master URL of the Spark cluster (local, yarn, etc.).\n",
    "print(sc.master)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "96400807-0383-4186-9ee4-39fbdfdf468c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('spark.databricks.preemption.enabled', 'true'), ('spark.sql.hive.metastore.jars', '/databricks/databricks-hive/*'), ('spark.driver.tempDirectory', '/local_disk0/tmp'), ('spark.sql.warehouse.dir', 'dbfs:/user/hive/warehouse'), ('spark.driver.host', '10.172.224.178'), ('spark.databricks.managedCatalog.clientClassName', 'com.databricks.managedcatalog.ManagedCatalogClientImpl'), ('spark.databricks.credential.scope.fs.gs.auth.access.tokenProviderClassName', 'com.databricks.backend.daemon.driver.credentials.CredentialScopeGCPTokenProvider'), ('spark.hadoop.fs.fcfs-s3.impl.disable.cache', 'true'), ('spark.sql.streaming.checkpointFileManagerClass', 'com.databricks.spark.sql.streaming.DatabricksCheckpointFileManager'), ('spark.databricks.service.dbutils.repl.backend', 'com.databricks.dbconnect.ReplDBUtils'), ('spark.hadoop.databricks.s3.verifyBucketExists.enabled', 'false'), ('spark.streaming.driver.writeAheadLog.allowBatching', 'true'), ('spark.databricks.clusterSource', 'UI'), ('spark.hadoop.hive.server2.transport.mode', 'http'), ('spark.databricks.acl.dfAclsEnabled', 'false'), ('spark.executor.memory', '8278m'), ('spark.hadoop.spark.driverproxy.customHeadersToProperties', 'X-Databricks-User-Token:spark.databricks.token,X-Databricks-Non-UC-User-Token:spark.databricks.non.uc.token,X-Databricks-Api-Url:spark.databricks.api.url,X-Databricks-ADLS-Gen1-Token:spark.databricks.adls.gen1.token,X-Databricks-ADLS-Gen2-Token:spark.databricks.adls.gen2.token,X-Databricks-Synapse-Token:spark.databricks.synapse.token,X-Databricks-AWS-Credentials:spark.databricks.aws.creds,X-Databricks-User-Id:spark.databricks.user.id,X-Databricks-User-Name:spark.databricks.user.name,X-Databricks-Oauth-Identity-Custom-Claim:spark.databricks.oauthCustomIdentityClaims,X-Databricks-Workload-Id:spark.databricks.workload.id,X-Databricks-Workload-Class:spark.databricks.workload.name'), ('spark.hadoop.fs.cpfs-adl.impl.disable.cache', 'true'), ('spark.repl.class.outputDir', '/local_disk0/tmp/repl/spark-161835142813421306-ad55184c-cf41-4559-add7-92f2e8fb5aa6'), ('spark.databricks.clusterUsageTags.hailEnabled', 'false'), ('spark.databricks.clusterUsageTags.clusterLogDeliveryEnabled', 'false'), ('spark.databricks.clusterUsageTags.containerType', 'LXC'), ('spark.hadoop.fs.s3a.assumed.role.credentials.provider', 'shaded.databricks.org.apache.hadoop.fs.s3a.DatabricksInstanceProfileCredentialsProvider'), ('spark.eventLog.enabled', 'false'), ('spark.hadoop.fs.stage.impl.disable.cache', 'true'), ('spark.hadoop.hive.hmshandler.retry.interval', '2000'), ('spark.executor.tempDirectory', '/local_disk0/tmp'), ('spark.hadoop.fs.azure.authorization.caching.enable', 'false'), ('spark.hadoop.fs.fcfs-abfss.impl', 'com.databricks.sql.acl.fs.FixedCredentialsFileSystem'), ('spark.hadoop.mapred.output.committer.class', 'com.databricks.backend.daemon.data.client.DirectOutputCommitter'), ('spark.hadoop.hive.server2.thrift.http.port', '10000'), ('spark.hadoop.mapreduce.fileoutputcommitter.algorithm.version', '2'), ('spark.databricks.clusterUsageTags.numPerClusterInitScriptsV2S3', '0'), ('spark.sql.allowMultipleContexts', 'false'), ('spark.databricks.eventLog.enabled', 'true'), ('spark.home', '/databricks/spark'), ('spark.databricks.clusterUsageTags.clusterTargetWorkers', '0'), ('spark.hadoop.hive.server2.idle.operation.timeout', '7200000'), ('spark.task.reaper.enabled', 'true'), ('spark.databricks.clusterUsageTags.autoTerminationMinutes', '60'), ('spark.storage.memoryFraction', '0.5'), ('spark.databricks.clusterUsageTags.clusterFirstOnDemand', '0'), ('spark.databricks.sql.configMapperClass', 'com.databricks.dbsql.config.SqlConfigMapperBridge'), ('spark.driver.maxResultSize', '4g'), ('spark.databricks.clusterUsageTags.sparkEnvVarContainsNewline', 'false'), ('spark.hadoop.fs.fcfs-s3.impl', 'com.databricks.sql.acl.fs.FixedCredentialsFileSystem'), ('spark.databricks.delta.multiClusterWrites.enabled', 'true'), ('spark.worker.cleanup.enabled', 'false'), ('spark.sql.legacy.createHiveTableByDefault', 'false'), ('spark.databricks.driver.preferredMavenCentralMirrorUrl', 'https://maven-central.storage-download.googleapis.com/maven2/'), ('spark.databricks.clusterUsageTags.numPerClusterInitScriptsV2File', '0'), ('spark.hadoop.fs.fcfs-s3a.impl.disable.cache', 'true'), ('spark.ui.port', '40001'), ('spark.hadoop.fs.s3a.attempts.maximum', '10'), ('spark.databricks.clusterUsageTags.enableCredentialPassthrough', 'false'), ('spark.databricks.clusterUsageTags.sparkEnvVarContainsDollarSign', 'false'), ('spark.databricks.clusterUsageTags.userProvidedRemoteVolumeType', 'ebs_volume_type: GENERAL_PURPOSE_SSD\\n'), ('spark.databricks.clusterUsageTags.enableJdbcAutoStart', 'true'), ('spark.databricks.clusterUsageTags.clusterId', '0121-050458-43thl78k'), ('spark.hadoop.fs.azure.user.agent.prefix', ''), ('spark.hadoop.fs.s3n.impl.disable.cache', 'true'), ('spark.databricks.clusterUsageTags.enableGlueCatalogCredentialPassthrough', 'false'), ('spark.hadoop.fs.fcfs-s3n.impl', 'com.databricks.sql.acl.fs.FixedCredentialsFileSystem'), ('spark.hadoop.fs.abfs.impl', 'com.databricks.common.filesystem.LokiFileSystem'), ('spark.hadoop.fs.s3a.retry.throttle.interval', '500ms'), ('spark.hadoop.fs.wasb.impl.disable.cache', 'true'), ('spark.databricks.clusterUsageTags.clusterLogDestination', ''), ('spark.databricks.wsfsPublicPreview', 'true'), ('spark.cleaner.referenceTracking.blocking', 'false'), ('spark.databricks.clusterUsageTags.clusterState', 'Pending'), ('spark.databricks.clusterUsageTags.sparkEnvVarContainsSingleQuotes', 'false'), ('spark.databricks.tahoe.logStore.azure.class', 'com.databricks.tahoe.store.AzureLogStore'), ('spark.repl.class.uri', 'spark://10.172.224.178:34803/classes'), ('spark.databricks.clusterUsageTags.clusterOwnerOrgId', '7820123208085273'), ('spark.databricks.clusterUsageTags.driverContainerPrivateIp', '10.172.224.178'), ('spark.hadoop.fs.azure.skip.metrics', 'true'), ('spark.hadoop.hive.hmshandler.retry.attempts', '10'), ('spark.scheduler.mode', 'FAIR'), ('spark.sql.sources.default', 'delta'), ('spark.databricks.unityCatalog.credentialManager.tokenRefreshEnabled', 'true'), ('spark.hadoop.fs.cpfs-s3n.impl', 'com.databricks.sql.acl.fs.CredentialPassthroughFileSystem'), ('spark.databricks.clusterUsageTags.clusterWorkers', '0'), ('spark.hadoop.fs.cpfs-adl.impl', 'com.databricks.sql.acl.fs.CredentialPassthroughFileSystem'), ('spark.databricks.sparkContextId', '161835142813421306'), ('spark.hadoop.fs.fcfs-s3n.impl.disable.cache', 'true'), ('spark.hadoop.fs.cpfs-abfss.impl', 'com.databricks.sql.acl.fs.CredentialPassthroughFileSystem'), ('spark.databricks.rocksDB.fileManager.useCommitService', 'false'), ('spark.databricks.passthrough.oauth.refresher.impl', 'com.databricks.backend.daemon.driver.credentials.OAuthTokenRefresherClient'), ('spark.hadoop.databricks.loki.fileStatusCache.gcs.enabled', 'false'), ('spark.sql.hive.metastore.sharedPrefixes', 'org.mariadb.jdbc,com.mysql.jdbc,org.postgresql,com.microsoft.sqlserver,microsoft.sql.DateTimeOffset,microsoft.sql.Types,com.databricks,com.codahale,com.fasterxml.jackson,shaded.databricks'), ('spark.databricks.io.directoryCommit.enableLogicalDelete', 'false'), ('spark.task.reaper.killTimeout', '60s'), ('spark.databricks.clusterUsageTags.attribute_tag_dust_runner', ''), ('spark.hadoop.parquet.block.size.row.check.min', '10'), ('spark.hadoop.hive.server2.use.SSL', 'true'), ('spark.databricks.clusterUsageTags.clusterAvailability', 'ON_DEMAND'), ('spark.databricks.clusterUsageTags.userProvidedRemoteVolumeSizeGb', '0'), ('spark.databricks.clusterUsageTags.attribute_tag_platform_name', ''), ('spark.hadoop.hive.server2.keystore.path', '/databricks/keys/jetty-ssl-driver-keystore.jks'), ('spark.databricks.deltaSharing.clientClassName', 'com.databricks.deltasharing.DataSharingClientImpl'), ('spark.hadoop.fs.gs.impl', 'com.databricks.common.filesystem.LokiFileSystem'), ('spark.app.id', 'local-1737435944224'), ('spark.databricks.credential.redactor', 'com.databricks.logging.secrets.CredentialRedactorProxyImpl'), ('spark.databricks.clusterUsageTags.clusterPinned', 'false'), ('spark.databricks.acl.provider', 'com.databricks.sql.acl.ReflectionBackedAclProvider'), ('spark.databricks.wsfs.workspacePrivatePreview', 'true'), ('spark.databricks.mlflow.autologging.enabled', 'true'), ('spark.extraListeners', 'com.databricks.backend.daemon.driver.DBCEventLoggingListener'), ('spark.databricks.clusterUsageTags.clusterAllTags', '[{\"key\":\"Name\",\"value\":\"ce-worker\"},{\"key\":\"WorkspaceId\",\"value\":\"7820123208085273\"},{\"key\":\"ClusterId\",\"value\":\"0121-050458-43thl78k\"}]'), ('spark.hadoop.spark.databricks.io.parquet.verifyChecksumOnWrite.enabled', 'false'), ('spark.sql.parquet.cacheMetadata', 'true'), ('spark.databricks.clusterUsageTags.numPerGlobalInitScriptsV2', '0'), ('spark.databricks.clusterUsageTags.numPerClusterInitScriptsV2Abfss', '0'), ('spark.hadoop.parquet.abfs.readahead.optimization.enabled', 'true'), ('spark.hadoop.fs.adl.impl', 'com.databricks.adl.AdlFileSystem'), ('spark.databricks.clusterUsageTags.driverContainerId', '3bcf98da7ecc43868ebf00bf893e015f'), ('spark.hadoop.fs.cpfs-abfss.impl.disable.cache', 'true'), ('spark.hadoop.fs.abfss.impl', 'com.databricks.common.filesystem.LokiFileSystem'), ('spark.databricks.clusterUsageTags.enableLocalDiskEncryption', 'false'), ('spark.databricks.tahoe.logStore.class', 'com.databricks.tahoe.store.DelegatingLogStore'), ('spark.hadoop.fs.s3.impl.disable.cache', 'true'), ('spark.hadoop.spark.hadoop.aws.glue.cache.db.ttl-mins', '30'), ('spark.hadoop.spark.hadoop.aws.glue.cache.table.ttl-mins', '30'), ('libraryDownload.sleepIntervalSeconds', '5'), ('spark.databricks.cloudProvider', 'AWS'), ('spark.sql.hive.convertMetastoreParquet', 'true'), ('spark.executor.id', 'driver'), ('spark.databricks.service.dbutils.server.backend', 'com.databricks.dbconnect.SparkServerDBUtils'), ('spark.databricks.clusterUsageTags.workerEnvironmentId', 'default-worker-env'), ('spark.databricks.repl.enableClassFileCleanup', 'true'), ('spark.hadoop.fs.s3a.multipart.size', '10485760'), ('spark.databricks.clusterUsageTags.cloudProvider', 'AWS'), ('spark.metrics.conf', '/databricks/spark/conf/metrics.properties'), ('spark.databricks.clusterUsageTags.driverPublicDns', 'ec2-34-222-247-170.us-west-2.compute.amazonaws.com'), ('spark.akka.frameSize', '256'), ('spark.hadoop.fs.s3a.fast.upload', 'true'), ('spark.hadoop.fs.wasbs.impl', 'shaded.databricks.org.apache.hadoop.fs.azure.NativeAzureFileSystem'), ('spark.sql.streaming.stopTimeout', '15s'), ('spark.hadoop.hive.server2.keystore.password', '[REDACTED]'), ('spark.databricks.clusterUsageTags.ignoreTerminationEventInAlerting', 'false'), ('spark.hadoop.fs.s3a.retry.interval', '250ms'), ('spark.databricks.clusterUsageTags.sparkEnvVarContainsEscape', 'false'), ('spark.databricks.overrideDefaultCommitProtocol', 'org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol'), ('spark.worker.aioaLazyConfig.dbfsReadinessCheckClientClass', 'com.databricks.backend.daemon.driver.NephosDbfsReadinessCheckClient'), ('spark.databricks.clusterUsageTags.clusterNoDriverDaemon', 'false'), ('libraryDownload.timeoutSeconds', '180'), ('spark.hadoop.parquet.memory.pool.ratio', '0.5'), ('spark.databricks.clusterUsageTags.shardName', 'devtierprod1'), ('spark.databricks.clusterUsageTags.clusterScalingType', 'fixed_size'), ('spark.hadoop.databricks.loki.fileStatusCache.abfs.enabled', 'false'), ('spark.databricks.passthrough.adls.gen2.tokenProviderClassName', 'com.databricks.backend.daemon.data.client.adl.AdlGen2CredentialContextTokenProvider'), ('spark.hadoop.fs.s3a.block.size', '67108864'), ('spark.databricks.clusterUsageTags.orgId', '7820123208085273'), ('spark.databricks.tahoe.logStore.gcp.class', 'com.databricks.tahoe.store.GCPLogStore'), ('spark.serializer.objectStreamReset', '100'), ('spark.databricks.clusterUsageTags.sparkMasterUrlType', 'None'), ('spark.databricks.passthrough.enabled', 'false'), ('spark.sql.sources.commitProtocolClass', 'com.databricks.sql.transaction.directory.DirectoryAtomicCommitProtocol'), ('spark.databricks.clusterUsageTags.numPerClusterInitScriptsV2Gcs', '0'), ('spark.databricks.clusterUsageTags.effectiveSparkVersion', '12.2.x-scala2.12'), ('spark.databricks.clusterUsageTags.attribute_tag_budget', ''), ('spark.hadoop.fs.fcfs-s3a.impl', 'com.databricks.sql.acl.fs.FixedCredentialsFileSystem'), ('spark.databricks.clusterUsageTags.currentAttemptContainerZoneId', 'us-west-2c'), ('spark.databricks.clusterUsageTags.clusterPythonVersion', '3'), ('spark.databricks.clusterUsageTags.enableDfAcls', 'false'), ('spark.databricks.clusterUsageTags.userProvidedRemoteVolumeCount', '0'), ('spark.hadoop.databricks.loki.fileSystemCache.enabled', 'true'), ('spark.shuffle.service.enabled', 'true'), ('spark.hadoop.fs.file.impl', 'com.databricks.backend.daemon.driver.WorkspaceLocalFileSystem'), ('spark.plugins', 'org.apache.spark.sql.connect.SparkConnectPlugin'), ('spark.hadoop.fs.fcfs-wasb.impl.disable.cache', 'true'), ('spark.hadoop.fs.cpfs-s3.impl', 'com.databricks.sql.acl.fs.CredentialPassthroughFileSystem'), ('spark.databricks.clusterUsageTags.containerZoneId', 'auto'), ('spark.databricks.clusterUsageTags.attribute_tag_dust_maintainer', ''), ('spark.hadoop.fs.s3a.multipart.threshold', '104857600'), ('spark.rpc.message.maxSize', '256'), ('spark.databricks.clusterUsageTags.attribute_tag_dust_suite', ''), ('spark.hadoop.fs.fcfs-wasbs.impl', 'com.databricks.sql.acl.fs.FixedCredentialsFileSystem'), ('spark.databricks.driverNfs.enabled', 'true'), ('spark.databricks.clusterUsageTags.clusterMetastoreAccessType', 'RDS_DIRECT'), ('spark.databricks.clusterUsageTags.ngrokNpipEnabled', 'false'), ('spark.hadoop.parquet.page.metadata.validation.enabled', 'true'), ('spark.databricks.acl.enabled', 'false'), ('spark.databricks.clusterUsageTags.instanceProfileUsed', 'false'), ('spark.databricks.unityCatalog.credentialManager.apiTokenProviderClassName', 'com.databricks.unity.TokenServiceApiTokenProvider'), ('spark.databricks.passthrough.glue.executorServiceFactoryClassName', 'com.databricks.backend.daemon.driver.credentials.GlueClientExecutorServiceFactory'), ('spark.app.startTime', '1737435937736'), ('spark.databricks.clusterUsageTags.awsWorkspaceIMDSV2EnablementStatus', 'false'), ('spark.databricks.clusterUsageTags.attribute_tag_dust_resource_class', ''), ('spark.databricks.acl.scim.client', 'com.databricks.spark.sql.acl.client.DriverToWebappScimClient'), ('spark.databricks.clusterUsageTags.sparkEnvVarContainsBacktick', 'false'), ('spark.databricks.clusterUsageTags.clusterName', \"Sachin Saxena's Cluster\"), ('spark.databricks.clusterUsageTags.isSingleUserCluster', 'true'), ('spark.hadoop.fs.adl.impl.disable.cache', 'true'), ('spark.hadoop.parquet.block.size.row.check.max', '10'), ('spark.hadoop.fs.s3a.connection.maximum', '200'), ('spark.databricks.clusterUsageTags.numPerClusterInitScriptsV2', '0'), ('spark.hadoop.fs.s3a.fast.upload.active.blocks', '32'), ('spark.shuffle.reduceLocality.enabled', 'false'), ('spark.databricks.clusterUsageTags.driverNodeType', 'dev-tier-node'), ('spark.hadoop.spark.sql.sources.outputCommitterClass', 'com.databricks.backend.daemon.data.client.MapReduceDirectOutputCommitter'), ('spark.hadoop.fs.fcfs-abfs.impl', 'com.databricks.sql.acl.fs.FixedCredentialsFileSystem'), ('spark.databricks.clusterUsageTags.instanceBootstrapType', 'ssh'), ('spark.hadoop.fs.fcfs-abfss.impl.disable.cache', 'true'), ('spark.hadoop.hive.server2.thrift.http.cookie.auth.enabled', 'false'), ('spark.hadoop.spark.hadoop.aws.glue.cache.table.size', '1000'), ('spark.databricks.driverNodeTypeId', 'dev-tier-node'), ('spark.sql.parquet.compression.codec', 'snappy'), ('spark.hadoop.fs.stage.impl', 'com.databricks.backend.daemon.driver.managedcatalog.PersonalStagingFileSystem'), ('spark.databricks.credential.scope.fs.s3a.tokenProviderClassName', 'com.databricks.backend.daemon.driver.credentials.CredentialScopeS3TokenProvider'), ('spark.databricks.cloudfetch.hasRegionSupport', 'true'), ('spark.hadoop.databricks.s3.create.deleteUnnecessaryFakeDirectories', 'false'), ('spark.hadoop.fs.wasb.impl', 'shaded.databricks.org.apache.hadoop.fs.azure.NativeAzureFileSystem'), ('spark.hadoop.spark.hadoop.aws.glue.cache.db.size', '1000'), ('spark.databricks.unityCatalog.enabled', 'false'), ('spark.databricks.workerNodeTypeId', 'dev-tier-node'), ('spark.databricks.passthrough.glue.credentialsProviderFactoryClassName', 'com.databricks.backend.daemon.driver.credentials.DatabricksCredentialProviderFactory'), ('spark.databricks.clusterUsageTags.clusterEbsVolumeSize', '0'), ('spark.sparklyr-backend.threads', '1'), ('spark.hadoop.fs.fcfs-wasb.impl', 'com.databricks.sql.acl.fs.FixedCredentialsFileSystem'), ('spark.databricks.passthrough.s3a.tokenProviderClassName', 'com.databricks.backend.daemon.driver.aws.AwsCredentialContextTokenProvider'), ('spark.databricks.session.share', 'false'), ('spark.databricks.clusterUsageTags.clusterResourceClass', 'default'), ('spark.databricks.isShieldWorkspace', 'false'), ('spark.hadoop.fs.idbfs.impl', 'com.databricks.io.idbfs.IdbfsFileSystem'), ('spark.driver.extraJavaOptions', '-XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED'), ('spark.databricks.cloudfetch.requestDownloadUrlsWithHeaders', 'false'), ('spark.databricks.telemetry.prometheus.samplingRate', '100'), ('spark.hadoop.fs.dbfs.impl', 'com.databricks.backend.daemon.data.client.DbfsHadoop3'), ('spark.databricks.clusterUsageTags.clusterSku', 'STANDARD_SKU'), ('spark.hadoop.fs.gs.impl.disable.cache', 'true'), ('spark.databricks.privateLinkEnabled', 'false'), ('spark.delta.sharing.profile.provider.class', 'io.delta.sharing.DeltaSharingCredentialsProvider'), ('spark.executor.extraJavaOptions', '-XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED -Djava.io.tmpdir=/local_disk0/tmp -XX:ReservedCodeCacheSize=512m -XX:+UseCodeCacheFlushing -XX:PerMethodRecompilationCutoff=-1 -XX:PerBytecodeRecompilationCutoff=-1 -Djava.security.properties=/databricks/spark/dbconf/java/extra.security -XX:-UseContainerSupport -XX:+PrintFlagsFinal -XX:+PrintGCDateStamps -XX:+PrintGCDetails -verbose:gc -Xss4m -Djava.library.path=/usr/java/packages/lib/amd64:/usr/lib64:/lib64:/lib:/usr/lib:/usr/lib/x86_64-linux-gnu/jni:/lib/x86_64-linux-gnu:/usr/lib/x86_64-linux-gnu:/usr/lib/jni -Djavax.xml.datatype.DatatypeFactory=com.sun.org.apache.xerces.internal.jaxp.datatype.DatatypeFactoryImpl -Djavax.xml.parsers.DocumentBuilderFactory=com.sun.org.apache.xerces.internal.jaxp.DocumentBuilderFactoryImpl -Djavax.xml.parsers.SAXParserFactory=com.sun.org.apache.xerces.internal.jaxp.SAXParserFactoryImpl -Djavax.xml.validation.SchemaFactory:http://www.w3.org/2001/XMLSchema=com.sun.org.apache.xerces.internal.jaxp.validation.XMLSchemaFactory -Dorg.xml.sax.driver=com.sun.org.apache.xerces.internal.parsers.SAXParser -Dorg.w3c.dom.DOMImplementationSourceList=com.sun.org.apache.xerces.internal.dom.DOMXSImplementationSourceImpl -Djavax.net.ssl.sessionCacheSize=10000 -Dscala.reflect.runtime.disable.typetag.cache=true -Dcom.google.cloud.spark.bigquery.repackaged.io.netty.tryReflectionSetAccessible=true -Dlog4j2.formatMsgNoLookups=true -Ddatabricks.serviceName=spark-executor-1'), ('spark.databricks.clusterUsageTags.isGroupCluster', 'false'), ('spark.driver.port', '34803'), ('spark.worker.aioaLazyConfig.iamReadinessCheckClientClass', 'com.databricks.backend.daemon.driver.NephosIamRoleCheckClient'), ('spark.databricks.clusterUsageTags.clusterEbsVolumeType', 'GENERAL_PURPOSE_SSD'), ('spark.databricks.automl.serviceEnabled', 'true'), ('spark.hadoop.parquet.page.size.check.estimate', 'false'), ('spark.databricks.clusterUsageTags.attribute_tag_service', ''), ('spark.databricks.passthrough.s3a.threadPoolExecutor.factory.class', 'com.databricks.backend.daemon.driver.aws.S3APassthroughThreadPoolExecutorFactory'), ('spark.databricks.metrics.filesystem_io_metrics', 'true'), ('spark.databricks.cloudfetch.requesterClassName', 'com.databricks.spark.sql.cloudfetch.DataDaemonCloudPresignedUrlRequester'), ('spark.master', 'local[8]'), ('spark.databricks.delta.logStore.crossCloud.fatal', 'true'), ('spark.databricks.driverNfs.clusterWidePythonLibsEnabled', 'true'), ('spark.files.fetchFailure.unRegisterOutputOnHost', 'true'), ('spark.databricks.clusterUsageTags.enableSqlAclsOnly', 'false'), ('spark.databricks.clusterUsageTags.clusterEbsVolumeCount', '0'), ('spark.databricks.clusterUsageTags.clusterSizeType', 'VM_CONTAINER'), ('spark.hadoop.databricks.fs.perfMetrics.enable', 'true'), ('spark.databricks.clusterUsageTags.clusterNumSshKeys', '0'), ('spark.hadoop.fs.gs.outputstream.upload.chunk.size', '16777216'), ('spark.databricks.tahoe.logStore.aws.class', 'com.databricks.tahoe.store.S3LockBasedLogStore'), ('spark.speculation.quantile', '0.9'), ('spark.databricks.clusterUsageTags.privateLinkEnabled', 'false'), ('spark.shuffle.manager', 'SORT'), ('spark.files.overwrite', 'true'), ('spark.databricks.credential.aws.secretKey.redactor', 'com.databricks.spark.util.AWSSecretKeyRedactorProxy'), ('spark.databricks.clusterUsageTags.clusterNumCustomTags', '0'), ('spark.hadoop.fs.s3.impl', 'com.databricks.common.filesystem.LokiFileSystem'), ('spark.hadoop.fs.s3a.impl.disable.cache', 'true'), ('spark.databricks.clusterUsageTags.sparkEnvVarContainsDoubleQuotes', 'false'), ('spark.r.numRBackendThreads', '1'), ('spark.hadoop.fs.wasbs.impl.disable.cache', 'true'), ('spark.hadoop.fs.abfss.impl.disable.cache', 'true'), ('spark.hadoop.fs.azure.cache.invalidator.type', 'com.databricks.encryption.utils.CacheInvalidatorImpl'), ('spark.sql.hive.metastore.version', '0.13.0'), ('spark.shuffle.service.port', '4048'), ('spark.databricks.clusterUsageTags.instanceWorkerEnvNetworkType', 'default'), ('spark.databricks.acl.client', 'com.databricks.spark.sql.acl.client.SparkSqlAclClient'), ('spark.streaming.driver.writeAheadLog.closeFileAfterWrite', 'true'), ('spark.hadoop.hive.warehouse.subdir.inherit.perms', 'false'), ('spark.databricks.clusterUsageTags.attribute_tag_dust_runbot_id', ''), ('spark.databricks.clusterUsageTags.userId', '7876100276558541'), ('spark.databricks.clusterUsageTags.runtimeEngine', 'STANDARD'), ('spark.databricks.clusterUsageTags.isServicePrincipalCluster', 'false'), ('spark.databricks.credential.scope.fs.impl', 'com.databricks.sql.acl.fs.CredentialScopeFileSystem'), ('spark.hadoop.databricks.loki.fileStatusCache.s3a.enabled', 'false'), ('spark.databricks.enablePublicDbfsFuse', 'false'), ('spark.databricks.clusterUsageTags.enableElasticDisk', 'false'), ('spark.hadoop.fs.fcfs-wasbs.impl.disable.cache', 'true'), ('spark.databricks.clusterUsageTags.userProvidedSparkVersion', '12.2.x-scala2.12'), ('spark.databricks.clusterUsageTags.clusterNodeType', 'dev-tier-node'), ('spark.databricks.passthrough.adls.tokenProviderClassName', 'com.databricks.backend.daemon.data.client.adl.AdlCredentialContextTokenProvider'), ('spark.app.name', 'Databricks Shell'), ('spark.driver.allowMultipleContexts', 'false'), ('spark.databricks.clusterUsageTags.sparkImageLabel', 'release__12.2.x-snapshot-scala2.12__databricks__12.2.40__0620907__c05f292__jenkins__745f7a2__format-3'), ('spark.hadoop.fs.AbstractFileSystem.gs.impl', 'shaded.databricks.com.google.cloud.hadoop.fs.gcs.GoogleHadoopFS'), ('spark.databricks.secret.sparkConf.keys.toRedact', ''), ('spark.rdd.compress', 'true'), ('spark.hadoop.spark.databricks.io.parquet.verifyChecksumOnWrite.throwsException', 'false'), ('spark.databricks.python.defaultPythonRepl', 'ipykernel'), ('spark.hadoop.fs.s3a.retry.limit', '6'), ('spark.databricks.clusterUsageTags.attribute_tag_dust_execution_env', ''), ('spark.databricks.clusterUsageTags.isIMv2Enabled', 'true'), ('spark.databricks.eventLog.dir', 'eventlogs'), ('spark.databricks.clusterUsageTags.isDpCpPrivateLinkEnabled', 'false'), ('spark.databricks.credential.scope.fs.adls.gen2.tokenProviderClassName', 'com.databricks.backend.daemon.driver.credentials.CredentialScopeADLSTokenProvider'), ('spark.databricks.driverNfs.pathSuffix', '.ephemeral_nfs'), ('spark.databricks.clusterUsageTags.clusterCreator', 'Webapp'), ('spark.speculation', 'false'), ('spark.hadoop.databricks.dbfs.client.version', 'v1'), ('spark.hadoop.hive.server2.session.check.interval', '60000'), ('spark.sql.hive.convertCTAS', 'true'), ('spark.hadoop.fs.s3a.max.total.tasks', '1000'), ('spark.hadoop.spark.sql.parquet.output.committer.class', 'org.apache.spark.sql.parquet.DirectParquetOutputCommitter'), ('spark.databricks.clusterUsageTags.sparkVersion', '12.2.x-scala2.12'), ('spark.databricks.clusterUsageTags.driverInstancePrivateIp', '10.172.245.230'), ('spark.hadoop.fs.s3a.fast.upload.default', 'true'), ('spark.databricks.clusterUsageTags.clusterGeneration', '0'), ('spark.hadoop.fs.mlflowdbfs.impl', 'com.databricks.mlflowdbfs.MlflowdbfsFileSystem'), ('spark.databricks.clusterUsageTags.clusterUnityCatalogMode', 'LEGACY_SINGLE_USER_STANDARD'), ('spark.databricks.eventLog.listenerClassName', 'com.databricks.backend.daemon.driver.DBCEventLoggingListener'), ('spark.hadoop.fs.abfs.impl.disable.cache', 'true'), ('spark.speculation.multiplier', '3'), ('spark.r.sql.derby.temp.dir', '/tmp/RtmpUJoCVn'), ('spark.storage.blockManagerTimeoutIntervalMs', '300000'), ('spark.databricks.clusterUsageTags.instanceWorkerEnvId', 'default-worker-env'), ('spark.sparkr.use.daemon', 'false'), ('spark.scheduler.listenerbus.eventqueue.capacity', '20000'), ('spark.hadoop.fs.s3a.impl', 'com.databricks.common.filesystem.LokiFileSystem'), ('spark.databricks.clusterUsageTags.clusterStateMessage', 'Starting Spark'), ('spark.hadoop.parquet.page.write-checksum.enabled', 'true'), ('spark.hadoop.databricks.s3commit.client.sslTrustAll', 'false'), ('spark.hadoop.fs.s3a.threads.max', '136'), ('spark.r.backendConnectionTimeout', '604800'), ('spark.databricks.clusterUsageTags.numPerClusterInitScriptsV2Dbfs', '0'), ('spark.hadoop.fs.s3n.impl', 'com.databricks.common.filesystem.LokiFileSystem'), ('spark.hadoop.hive.server2.idle.session.timeout', '900000'), ('spark.databricks.redactor', 'com.databricks.spark.util.DatabricksSparkLogRedactorProxy'), ('spark.databricks.clusterUsageTags.driverInstanceId', 'i-080dd1060c8796620'), ('spark.executor.extraClassPath', '/databricks/spark/dbconf/log4j/executor:/databricks/spark/dbconf/jets3t/:/databricks/spark/dbconf/hadoop:/databricks/hive/conf:/databricks/jars/*'), ('spark.databricks.autotune.maintenance.client.classname', 'com.databricks.maintenanceautocompute.MACClientImpl'), ('spark.databricks.clusterUsageTags.numPerClusterInitScriptsV2Volumes', '0'), ('spark.databricks.clusterUsageTags.numPerClusterInitScriptsV2Workspace', '0'), ('spark.hadoop.fs.fcfs-abfs.impl.disable.cache', 'true'), ('spark.hadoop.parquet.page.verify-checksum.enabled', 'true'), ('spark.databricks.clusterUsageTags.dataPlaneRegion', 'us-west-2'), ('spark.logConf', 'true'), ('spark.databricks.clusterUsageTags.enableJobsAutostart', 'true'), ('spark.databricks.clusterUsageTags.clusterOwnerUserId', '7876100276558541'), ('spark.hadoop.hive.server2.enable.doAs', 'false'), ('spark.hadoop.parquet.filter.columnindex.enabled', 'false'), ('spark.shuffle.memoryFraction', '0.2'), ('spark.hadoop.fs.dbfsartifacts.impl', 'com.databricks.backend.daemon.data.client.DBFSV1'), ('spark.hadoop.fs.cpfs-s3a.impl', 'com.databricks.sql.acl.fs.CredentialPassthroughFileSystem'), ('spark.databricks.clusterUsageTags.attribute_tag_dust_bazel_path', ''), ('spark.hadoop.fs.s3a.connection.timeout', '50000'), ('spark.databricks.secret.envVar.keys.toRedact', ''), ('spark.databricks.clusterUsageTags.region', 'us-west-2'), ('spark.databricks.clusterUsageTags.clusterSpotBidPricePercent', '100'), ('spark.files.useFetchCache', 'false')]\n"
     ]
    }
   ],
   "source": [
    "# Get Other Details: You can also use sc.getConf() to retrieve the configuration of the current SparkContext:\n",
    "print(sc.getConf().getAll())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ea7fec42-da95-48e0-bed7-c8082afe1a7a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "The spark context has stopped and the driver is restarting. Your notebook will be automatically reattached.",
       "errorTraceType": "html",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Reuse the Existing SparkContext: Avoid explicitly initializing SparkContext() and instead rely on getOrCreate() to use the existing one.\n",
    "# Stop the Existing SparkContext: If you need to create a new context, stop the current one first:\n",
    "sc.stop()\n",
    "from pyspark import SparkContext\n",
    "sc = SparkContext(appName=\"NewAppName\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "15d410ff-8561-4105-90f3-64166641a814",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Use Databricks-Managed SparkContext: If working in Databricks, avoid manually creating SparkContext, as Databricks provides a pre-initialized SparkSession:\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "511b0cc4-f10d-45e8-80c6-3a93a8920a9e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Initialize SparkSession\n",
    "# Ensure you have a SparkSession object, which is the entry point for working with DataFrames.\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create or get the SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Read CSV File Example\") \\\n",
    "    .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9c0bb21e-cf79-4742-a67a-eca62d86c327",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Read the CSV File\n",
    "# You can use the read.csv() method to load the CSV file into a DataFrame.\n",
    "# Read CSV file into DataFrame\n",
    "df = spark.read.csv(\"dbfs:/FileStore/streaming/Countries1.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3711fd55-fbec-4844-892d-4c8e08131437",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# With Header and Inferred Schema:\n",
    "# If the CSV file has a header and you want Spark to infer the schema:\n",
    "df = spark.read.format(\"csv\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "        .option(\"inferSchema\", \"true\") \\\n",
    "            .load(\"dbfs:/FileStore/streaming/Countries1.csv\")\n",
    "        \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8de03f9d-701d-46df-9eb2-cca75d5c1d3d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of partitions: 1\n"
     ]
    }
   ],
   "source": [
    "#  How many partitions do we have?\n",
    "# By default, the number of partitions is determined by the number of cores available\n",
    "# in your local setup or cluster.\n",
    "# If you are running it locally, it's often based on the number of CPU cores.\n",
    "# Get the number of partitions\n",
    "num_partitions = df.rdd.getNumPartitions()\n",
    "print(\"Number of partitions:\", num_partitions)\n",
    "# Explanation\n",
    "# df.rdd: Converts the DataFrame to its underlying RDD.\n",
    "# getNumPartitions(): Returns the number of partitions of the RDD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a3418c4f-5416-49e0-b9ee-97a407d0e545",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type of num_partitions: <class 'int'>\n"
     ]
    }
   ],
   "source": [
    "print(\"Type of num_partitions:\", type(num_partitions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "da5b5cfb-9101-4de1-b769-ab5791b9f58d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type of rdd: <class 'pyspark.rdd.RDD'>\n"
     ]
    }
   ],
   "source": [
    "rdd = df.rdd\n",
    "print(\"Type of rdd:\", type(rdd))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c76e03aa-dd7a-448d-973b-ec0f96832448",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[24]: [6]"
     ]
    }
   ],
   "source": [
    "rdd.glom().map(len).collect()\n",
    "\n",
    "# glom(): Transforms each partition of the RDD into a list. Instead of working with individual elements, you now have a list of elements for each partition.\n",
    "\n",
    "# map(len): Applies the len function to each partition (which is now a list) to get the count of elements in that partition.\n",
    "\n",
    "# collect(): Collects the result back to the driver as a list, giving the count of elements in each partition.\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b3c1ded9-0edc-4f5f-8bb9-c1ceaa91f744",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row(Country='India', Citizens=10), Row(Country='USA', Citizens=5), Row(Country='China', Citizens=10), Row(Country='India', Citizens=10), Row(Country='Canada', Citizens=40), Row(Country='Brazil', Citizens=10)]\n"
     ]
    }
   ],
   "source": [
    "# To check the contents of the RDD\n",
    "print(rdd.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7427b29b-98ae-44bf-bb64-1c6b93996825",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "rdd =rdd.repartition(10)\n",
    "\n",
    "# Can increase or decrease the level of parallelism in this RDD.\n",
    "# Internally, this uses a shuffle to redistribute data.\n",
    "# If you are decreasing the number of partitions in this RDD, consider using coalesce,\n",
    "#  which can avoid performing a shuffle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c058e0e2-bec9-407c-94eb-586021221c00",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[37]: [0, 0, 0, 0, 0, 0, 0, 0, 0, 6]"
     ]
    }
   ],
   "source": [
    "\n",
    "rdd.glom().map(len).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1e3202b6-f395-4016-8dc3-649800575bc2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n10\nOut[38]: MapPartitionsRDD[73] at coalesce at NativeMethodAccessorImpl.java:0"
     ]
    }
   ],
   "source": [
    "print(sc.defaultParallelism)\n",
    "print(rdd.getNumPartitions())\n",
    "\n",
    "rdd.persist()\n",
    "# 2 cores and 10 partitions, 5 partitions in each core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6dd215cf-0754-40a9-94c0-58f417d798c7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 6]\n"
     ]
    }
   ],
   "source": [
    "# The output of your rdd.glom().map(len).collect() indicates that your existing RDD is unevenly distributed across partitions, with 9 partitions having no data and 1 partition containing all the elements (6). This imbalance can affect performance in distributed processing. To address this, you can repartition the RDD to distribute the data more evenly.\n",
    "\n",
    "# Heres how you can parallelize and balance your existing RDD:\n",
    "# Repartition the RDD into a desired number of partitions (e.g., 10)\n",
    "balanced_rdd = rdd.repartition(10)\n",
    "\n",
    "# Verify the new partition sizes\n",
    "print(balanced_rdd.glom().map(len).collect())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "943d629e-1e61-4898-a734-e1de9a43f4ec",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[], [], [], [], [], [], [], [], [], [Row(Country='India', Citizens=10), Row(Country='USA', Citizens=5), Row(Country='China', Citizens=10), Row(Country='India', Citizens=10), Row(Country='Canada', Citizens=40), Row(Country='Brazil', Citizens=10)]]\n"
     ]
    }
   ],
   "source": [
    "# Inspect the data distribution across partitions\n",
    "print(rdd.glom().collect())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d8194695-7aa2-4cfe-b39f-c3687d823e45",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 0, 1, 1, 0, 1, 0, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "# You can explicitly recreate a balanced RDD using sparkContext.parallelize(). Heres how to assign and parallelize data properly:\n",
    "\n",
    "# Collect the RDD to a Python list\n",
    "data_list = rdd.collect()\n",
    "\n",
    "# Re-parallelize the data with the desired number of partitions\n",
    "balanced_rdd = spark.sparkContext.parallelize(data_list, 10)\n",
    "\n",
    "# Verify the new partition distribution\n",
    "print(balanced_rdd.glom().map(len).collect())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f9c37d5a-ec86-4f73-bf7a-3fcab3f6a25f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 6]\n"
     ]
    }
   ],
   "source": [
    "# Using Repartition with Larger Datasets\n",
    "# If your dataset is large enough, repartition() should naturally distribute it. However, for small datasets, repartitioning may still leave some partitions empty. Here's how to ensure shuffling:\n",
    "\n",
    "# Force shuffle and redistribute the data\n",
    "balanced_rdd = rdd.repartition(4)\n",
    "\n",
    "# Force an action to materialize the redistribution\n",
    "balanced_rdd.count()\n",
    "\n",
    "# Check the distribution again\n",
    "print(balanced_rdd.glom().map(len).collect())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "82d05de0-82a8-4b5c-a688-44af2e53de66",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original partition data: [[], [], [], [], [], [], [], [], [], [Row(Country='India', Citizens=10), Row(Country='USA', Citizens=5), Row(Country='China', Citizens=10), Row(Country='India', Citizens=10), Row(Country='Canada', Citizens=40), Row(Country='Brazil', Citizens=10)]]\nBalanced partition data: [[], [Row(Country='India', Citizens=10)], [], [Row(Country='USA', Citizens=5)], [Row(Country='China', Citizens=10)], [], [Row(Country='India', Citizens=10)], [], [Row(Country='Canada', Citizens=40)], [Row(Country='Brazil', Citizens=10)]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Original partition data:\", rdd.glom().collect())\n",
    "print(\"Balanced partition data:\", balanced_rdd.glom().collect())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4e16fe71-57ad-4529-8de7-4e2f1bf7bf68",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "India: [Row(Country='India', Citizens=10), Row(Country='India', Citizens=10)]\nUSA: [Row(Country='USA', Citizens=5)]\nBrazil: [Row(Country='Brazil', Citizens=10)]\nCanada: [Row(Country='Canada', Citizens=40)]\nChina: [Row(Country='China', Citizens=10)]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Group by the first element (name)\n",
    "grouped_rdd = balanced_rdd.groupBy(lambda x: x[0])\n",
    "\n",
    "# Process each group\n",
    "result = grouped_rdd.mapValues(lambda x: list(x)).collect()\n",
    "\n",
    "# Print the grouped data\n",
    "for key, value in result:\n",
    "    print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2ec31569-c7e9-42c0-8144-1399141497d2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Group by the first element (name)\n",
    "grouped_rdd = rdd.groupBy(lambda x: x[0])\n",
    "\n",
    "# Process each group\n",
    "result = grouped_rdd.mapValues(lambda x: list(x)).collect()\n",
    "\n",
    "# Print the grouped data\n",
    "for key, value in result:\n",
    "    print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f5a7ac87-75e1-4232-b71b-06172c80c85b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "\n",
    "emp_df=rdd.filter(col(\"Country\")).select().groupby"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1e36f1c9-2466-4b0e-a2bb-37b7b48884b5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grouped Data: [('India', 20), ('USA', 5), ('Brazil', 10), ('Canada', 40), ('China', 10)]\n"
     ]
    }
   ],
   "source": [
    "grouped_rdd = rdd.groupByKey().mapValues(sum)\n",
    "\n",
    "# Collect and print the grouped data\n",
    "print(\"Grouped Data:\", grouped_rdd.collect())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "233bc602-cdd2-427a-bf62-5c708ee9d06b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# With Header and Inferred Schema:\n",
    "# If the CSV file has a header and you want Spark to infer the schema:\n",
    "df = spark.read.format(\"csv\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "            .load(\"dbfs:/FileStore/streaming/Countries1.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2fefb890-f72b-4c3f-b5e4-e5a9e4891faf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "print(df.rdd.getNumPartitions())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b1d61b14-c916-4bff-aae9-69ba54426639",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "partition_df = df.repartition(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cf085183-5db2-47ba-ab3d-1f076be39097",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame[Country: string, Citizens: string]\n"
     ]
    }
   ],
   "source": [
    "print(partition_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f86a842b-5456-4f16-b64f-d6322b98d202",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n"
     ]
    }
   ],
   "source": [
    "print(partition_df.rdd.getNumPartitions())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "24e05fec-ae15-4e32-8ca7-9b430c1c55f3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[56]: [Row(Country='India', Citizens='10'), Row(Country='India', Citizens='10')]"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "partition_df=partition_df.filter(col(\"Country\")==\"India\")\n",
    "partition_df.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "11fbfd31-738e-4c13-9ffe-26bcb7f58f23",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[57]: [Row(Country='India', Citizens='10'), Row(Country='India', Citizens='10')]"
     ]
    }
   ],
   "source": [
    "\n",
    "partition_df=partition_df.filter(col(\"Country\")==\"India\")\\\n",
    "    .select(\"Country\",\"Citizens\")\n",
    "partition_df.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ead96c74-ff92-47e3-b599-85f6f88a0edc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[58]: [Row(Country='India', count=2)]"
     ]
    }
   ],
   "source": [
    "partition_df=partition_df.filter(col(\"Country\")==\"India\")\\\n",
    "    .select(\"Country\",\"Citizens\")\\\n",
    "        .groupby(\"Country\").count()\n",
    "partition_df.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "03695e49-94ee-48be-94db-f823560bab26",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "client": "1"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Day 10 encounter with error \"Cannot run multiple SparkContexts at once\" Graph DAG",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
